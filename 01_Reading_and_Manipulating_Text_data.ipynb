{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammad-Amirifard/NLP_Tutorial/blob/main/01_Reading_and_Manipulating_Text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Learn Natural Language Processing better in a practical Mode**\n",
        "\n",
        "Created By [Mohammad Amirifard](https://www.linkedin.com/in/mohammad-amirifard/)\n",
        "\n",
        "\n",
        "<img src=\"https://th.bing.com/th/id/R.481b9fae699f7619971167cade786c60?rik=PWShnQ5jIirVIA&riu=http%3a%2f%2fwww.marktechpost.com%2fwp-content%2fuploads%2f2022%2f11%2f233-scaled.jpg&ehk=hvDRc3N3UsK9xB5jAui%2b9p1VW98U8f6uUCoUncfmRbY%3d&risl=&pid=ImgRaw&r=0\" alt=\"Image\" width=\"800\" height=\"400\" />\n",
        "\n",
        "# **Structure of notebooks**\n",
        "`This NLP Tutorial includes several notebooks regarding different parts.`\n",
        "\n",
        "This is **notebook number 1**, called **Reading and Manipulating Text data**\n",
        "\n",
        "For other notebooks you can use the following links:\n",
        "\n",
        "\n",
        "1.   Notebook number 1, [Part1_EDA](https://github.com/Mohammad-Amirifard/Intrusion_Detection/blob/main/Notebooks/Part1_EDA.ipynb)\n",
        "2.   Notebook number 2, [Part2_KNN_Models](https://github.com/Mohammad-Amirifard/Intrusion_Detection/blob/main/Notebooks/Part2_KNN_Models.ipynb)\n",
        "3.   Notebook number 3, [Part3_SVM_Models](https://github.com/Mohammad-Amirifard/Intrusion_Detection/blob/main/Notebooks/Part3_SVM_Models.ipynb)\n",
        "4.   Notebook number 4, [Part4_Gboost_Model](https://github.com/Mohammad-Amirifard/Intrusion_Detection/blob/main/Notebooks/Part4_GBoost_Model.ipynb)\n",
        "5.   Notebook number 5, [Part5_MLP_Model](https://github.com/Mohammad-Amirifard/Intrusion_Detection/blob/main/Notebooks/Part5_MLP_Model.ipynb)\n",
        "6.   Notebook number 6, [Part6_Conclusion](https://github.com/Mohammad-Amirifard/Intrusion_Detection/blob/main/Notebooks/Part6_Conclusion.ipynb)"
      ],
      "metadata": {
        "id": "odVEn2zybWZh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9fk15RybKqy"
      },
      "source": [
        "#Load text from String\n",
        "\n",
        "In Python, a string is a sequence of characters used to represent textual data—such as a sentence, paragraph, or even an entire document. Strings are a fundamental data type that allow you to store and manipulate text efficiently.\n",
        "\n",
        "To illustrate this, let’s begin by creating a Python variable named `string_example` that holds a brief document in the form of a string. Once the variable is defined, we can simply type its name to display its contents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XzuBscVnbKqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2988d5-5cad-434b-9c86-6c28146dc039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created string is:\n",
            "Hello, I am Mohammad Amirifard. I am a student and want to create this tutorial to help myself and others in learning NLP\n"
          ]
        }
      ],
      "source": [
        "string_example = 'Hello, I am Mohammad Amirifard. I am a student and want to create this tutorial to help myself and others in learning NLP'\n",
        "print(\"Created string is:\")\n",
        "print(string_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## String length"
      ],
      "metadata": {
        "id": "hOyZfFYFeYvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B0KzLRCjbKq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288003a7-01b3-4bf0-f7b9-37f900b4c9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the created string is:\n",
            "121\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of the created string is:\")\n",
        "print(len(string_example))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len('I am'))  # This will return 4, as it counts 'I', ' ', 'a', and 'm'."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p-irZ0pfQoz",
        "outputId": "8fb0d589-aad7-464a-c94f-df930ef7e1fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlH3zCkHbKq0"
      },
      "source": [
        "## String split\n",
        "We can segment a sentence into individual words by splitting it based on whitespace (including spaces, tabs, etc.).\n",
        "\n",
        "In natural language processing (NLP), this process is referred to as **tokenization**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "826CWmTnbKq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9bdfae0-54cc-40b9-90a0-40aab8e2aa33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting the created string into words:\n",
            "['Hello,', 'I', 'am', 'Mohammad', 'Amirifard.', 'I', 'am', 'a', 'student', 'and', 'want', 'to', 'create', 'this', 'tutorial', 'to', 'help', 'myself', 'and', 'others', 'in', 'learning', 'NLP']\n"
          ]
        }
      ],
      "source": [
        "print(\"Splitting the created string based on the white space into words:\")\n",
        "print(string_example.split())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7VvLRsobKq1"
      },
      "source": [
        "The sentence need not be split solely by whitespace; it can also be divided using any arbitrary substring.\n",
        "\n",
        "For instance, one could split the text using a comma (',') as the delimiter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ahRXQem1bKq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b10905-1bea-4e13-e8ca-d017914037c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting the created string based on the comma into words:\n",
            "['Hello', ' I am Mohammad Amirifard. I am a student and want to create this tutorial to help myself and others in learning NLP']\n"
          ]
        }
      ],
      "source": [
        "print(\"Splitting the created string based on the comma into words:\")\n",
        "print(string_example.split(','))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Splitting the created string based on the dot into words:\")\n",
        "print(string_example.split('.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wzg7kSCg0iF",
        "outputId": "2acc5471-59a7-45a2-e50b-e0f4c710d338"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting the created string based on the dot into words:\n",
            "['Hello, I am Mohammad Amirifard', ' I am a student and want to create this tutorial to help myself and others in learning NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Length"
      ],
      "metadata": {
        "id": "z4fWcIzthAya"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04A8J71QbKq1"
      },
      "source": [
        "How many words are there in the document?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "03XDB4C1bKq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2aa48a-b565-431b-aafa-12b9385a0761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the created string(document) is:\n",
            "23\n"
          ]
        }
      ],
      "source": [
        "words = string_example.split()\n",
        "print(\"Number of words in the created string(document) is:\")\n",
        "print(len(words))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bwb2-TTbKq2"
      },
      "source": [
        "## Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-utEyCjZbKq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0540c374-fc7e-449f-a577-f3539b90cf13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noraml view of the created string is:\n",
            "Hello, I am Mohammad Amirifard. I am a student and want to create this tutorial to help myself and others in learning NLP\n",
            "****************************************************************************************************\n",
            "Lowercase of the created string is:\n",
            "hello, i am mohammad amirifard. i am a student and want to create this tutorial to help myself and others in learning nlp\n"
          ]
        }
      ],
      "source": [
        "print(\"Noraml view of the created string is:\")\n",
        "print(string_example)\n",
        "print(\"*\"*100)\n",
        "print(\"Lowercase of the created string is:\")\n",
        "print(string_example.lower())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgR1MpJWbKq2"
      },
      "source": [
        "# Loading Text from a File\n",
        "The following section demonstrates how to load a longer document from the text file 'One_Story.txt'.\n",
        "\n",
        "Prerequisites:\n",
        "1. Ensure the file \"One_Story.txt\" has been downloaded from the \"docs\" directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Zw9JB10EbKq2"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/docs/One_Story.txt\") as f:\n",
        "    document = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gPJhHrEbKq3"
      },
      "source": [
        "Print out the text as Python sees it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "27CsU1gbbKq3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "6547b2f7-197d-4753-d54d-77f62bc78ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Show the text without print command:\n",
            "****************************************************************************************************\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Last Letter\\n\\nThe train was late again.\\n\\nEli sat on the cold bench of platform 9, clutching a worn envelope in his coat pocket. His fingers trembled—not from the chill of the late autumn air, but from the weight of the words inside that envelope. He’d written the letter three weeks ago but hadn’t found the courage to send it. Today, he would hand it over in person.\\n\\nThe letter was for Lila.\\n\\nThey hadn’t spoken in over a year. Not since the argument, the slammed door, and the silence that followed like fog over a quiet lake. But Lila was coming home for her father’s memorial, and Eli had promised himself he’d try to make things right.\\n\\nThe train finally screeched in. Doors opened. People spilled out—families with suitcases, students with headphones, a man with a bouquet of roses. And then there she was.\\n\\nLila hadn’t changed much. Same short curls, same way she walked quickly like she was always late for something. But there was a heaviness in her eyes he hadn’t seen before. She didn’t see him yet.\\n\\nEli stood, heart pounding. He almost backed away, but then she looked up. Their eyes met.\\n\\nLila paused.\\n\\nHe gave her a small wave. She didn’t smile, but she didn’t turn away either.\\n\\n“Hey,” he said when she approached.\\n\\n“Hey,” she replied, cautious but calm.\\n\\nThere was a long pause. People swirled around them, but they stood still, caught in an invisible bubble of memories and unspoken words.\\n\\n“I wrote you something,” Eli said, pulling the envelope from his coat.\\n\\nLila looked at it but didn’t take it.\\n\\n“It’s not long,” he added. “But it’s honest.”\\n\\nShe took it slowly, eyes still on his face, not the letter. “Why now?”\\n\\n“Because I realized something. We said a lot of things we didn’t mean. Or maybe… we meant them in the moment, but not forever. I didn’t want silence to be the last thing between us.”\\n\\nLila looked down at the letter. Her thumb traced the edge of the paper.\\n\\n“I don’t expect anything,” Eli said. “Just wanted you to know I’m sorry. For all of it.”\\n\\nLila nodded, but didn’t speak. She slipped the envelope into her bag.\\n\\n“I should go,” she finally said.\\n\\nHe nodded. “Of course.”\\n\\nAs she turned to walk away, she stopped.\\n\\n“Thank you… for not sending it by mail,” she said over her shoulder.\\n\\nEli smiled, heart lighter.\\n\\nShe walked off into the crowd. He didn’t know if she would read it tonight, next week, or ever—but that didn’t matter. What mattered was that he tried.\\n\\nSometimes, all a story needs is a beginning, a wound, and the courage to write an ending—even if it’s not the one you hoped for.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "print('Show the text without print command:')\n",
        "print('*'*100)\n",
        "document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEDHeg0EbKq3"
      },
      "source": [
        "Note all the backslash characters '\\\\' in the text above.  \n",
        "- Python stores text as one big string (sequence of characters).\n",
        "- Special characters such as newlines and tabs are represented by '\\\\n' and '\\\\t' respectively.\n",
        "- The quote character is used to mark the start and end of the string ('string'), so quote characters that are present in the string are prefixed by a backslash to prevent the string from ending early ('str\\\\'ing').\n",
        "- Using the print() command, we can output the string in a format that we're more used to seeing it in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3m9q8V4QbKq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4913af-f0df-460a-8e4a-edc7f04baeee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Show the text using print command:\n",
            "****************************************************************************************************\n",
            "The Last Letter\n",
            "\n",
            "The train was late again.\n",
            "\n",
            "Eli sat on the cold bench of platform 9, clutching a worn envelope in his coat pocket. His fingers trembled—not from the chill of the late autumn air, but from the weight of the words inside that envelope. He’d written the letter three weeks ago but hadn’t found the courage to send it. Today, he would hand it over in person.\n",
            "\n",
            "The letter was for Lila.\n",
            "\n",
            "They hadn’t spoken in over a year. Not since the argument, the slammed door, and the silence that followed like fog over a quiet lake. But Lila was coming home for her father’s memorial, and Eli had promised himself he’d try to make things right.\n",
            "\n",
            "The train finally screeched in. Doors opened. People spilled out—families with suitcases, students with headphones, a man with a bouquet of roses. And then there she was.\n",
            "\n",
            "Lila hadn’t changed much. Same short curls, same way she walked quickly like she was always late for something. But there was a heaviness in her eyes he hadn’t seen before. She didn’t see him yet.\n",
            "\n",
            "Eli stood, heart pounding. He almost backed away, but then she looked up. Their eyes met.\n",
            "\n",
            "Lila paused.\n",
            "\n",
            "He gave her a small wave. She didn’t smile, but she didn’t turn away either.\n",
            "\n",
            "“Hey,” he said when she approached.\n",
            "\n",
            "“Hey,” she replied, cautious but calm.\n",
            "\n",
            "There was a long pause. People swirled around them, but they stood still, caught in an invisible bubble of memories and unspoken words.\n",
            "\n",
            "“I wrote you something,” Eli said, pulling the envelope from his coat.\n",
            "\n",
            "Lila looked at it but didn’t take it.\n",
            "\n",
            "“It’s not long,” he added. “But it’s honest.”\n",
            "\n",
            "She took it slowly, eyes still on his face, not the letter. “Why now?”\n",
            "\n",
            "“Because I realized something. We said a lot of things we didn’t mean. Or maybe… we meant them in the moment, but not forever. I didn’t want silence to be the last thing between us.”\n",
            "\n",
            "Lila looked down at the letter. Her thumb traced the edge of the paper.\n",
            "\n",
            "“I don’t expect anything,” Eli said. “Just wanted you to know I’m sorry. For all of it.”\n",
            "\n",
            "Lila nodded, but didn’t speak. She slipped the envelope into her bag.\n",
            "\n",
            "“I should go,” she finally said.\n",
            "\n",
            "He nodded. “Of course.”\n",
            "\n",
            "As she turned to walk away, she stopped.\n",
            "\n",
            "“Thank you… for not sending it by mail,” she said over her shoulder.\n",
            "\n",
            "Eli smiled, heart lighter.\n",
            "\n",
            "She walked off into the crowd. He didn’t know if she would read it tonight, next week, or ever—but that didn’t matter. What mattered was that he tried.\n",
            "\n",
            "Sometimes, all a story needs is a beginning, a wound, and the courage to write an ending—even if it’s not the one you hoped for.\n"
          ]
        }
      ],
      "source": [
        "print('Show the text using print command:')\n",
        "print('*'*100)\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiCwKJ_VbKq3"
      },
      "source": [
        "## Text Segmentation and Word Extraction\n",
        "\n",
        "The text can be divided into individual lines using the `splitlines()` method.\n",
        "- To maintain readability, only the first five lines will be displayed by applying the slice operation `[:5]` to the line-containing variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aLyhzsTQbKq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66fba14-8be0-42f5-9e91-c830fb75a4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines in the created string(document) is:\n",
            "55\n",
            "First five lines in the created string(document) is:\n",
            "****************************************************************************************************\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Last Letter',\n",
              " '',\n",
              " 'The train was late again.',\n",
              " '',\n",
              " 'Eli sat on the cold bench of platform 9, clutching a worn envelope in his coat pocket. His fingers trembled—not from the chill of the late autumn air, but from the weight of the words inside that envelope. He’d written the letter three weeks ago but hadn’t found the courage to send it. Today, he would hand it over in person.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "lines = document.splitlines()\n",
        "print(\"Number of lines in the created string(document) is:\")\n",
        "print(len(lines))\n",
        "print(\"First five lines in the created string(document) is:\")\n",
        "print(\"*\"*100)\n",
        "lines[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search a word in Doc"
      ],
      "metadata": {
        "id": "wd0PToYmlbnx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD9FbeMnbKq4"
      },
      "source": [
        "The text can be analyzed to locate specific lexical items.\n",
        "\n",
        "For demonstration purposes, we will search for the token '`train`' within the corpus.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "oRAD91wXbKq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4922501c-547e-4e74-fa9a-965ef2d29800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The index of the word 'train' is:\n",
            "21\n"
          ]
        }
      ],
      "source": [
        "word = 'train'\n",
        "print(\"The index of the word 'train' in the text is:\")\n",
        "print(document.find(word))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j34S1khxpIH3"
      },
      "source": [
        "What happens if we search for a string that does't exist in the document?\n",
        "- Try it..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "LCrROJ69pCa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6178ca5-9df2-4d5d-b420-a9c225876758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1\n"
          ]
        }
      ],
      "source": [
        "word = 'Laptop'\n",
        "print(document.find(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_1gK8GIbKq5"
      },
      "source": [
        "## Vocabulary Extraction\n",
        "To identify the unique lexical items in the text, we perform the following sequence of operations:\n",
        "\n",
        "\n",
        "1.   Convert all text to lowercase to ensure case-insensitive analysis\n",
        "2.   Tokenize the text by splitting on whitespace boundaries\n",
        "3.   Apply the `set()` function to extract only distinct word forms\n",
        "\n",
        "Note: In Python, sets implement the mathematical concept of sets, which:\n",
        "\n",
        "Can contain elements of heterogeneous types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "5zkUbepNbKq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e5b50c2-ae2d-4eff-ce79-0453b4624631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the created string(document) is:\n",
            "458\n",
            "Number of distinct words in the created string(document) is:\n",
            "259\n",
            "Distinct words in the created string(document) is:\n",
            "****************************************************************************************************\n",
            "{'stood,', 'you', 'promised', 'lila', 'sorry.', 'she', 'walk', 'approached.', 'it’s', 'honest.”', 'sometimes,', 'hand', 'all', 'year.', 'their', 'pause.', 'moment,', 'way', 'invisible', 'off', 'eli', 'eyes', 'expect', 'up.', '“why', 'much.', 'realized', 'or', 'almost', 'sending', 'worn', 'face,', 'one', 'sat', 'the', 'train', 'from', 'before.', 'maybe…', 'then', 'opened.', 'it', '“hey,”', 'bench', 'course.”', 'clutching', 'and', 'thing', 'either.', 'traced', 'them', 'replied,', 'right.', 'unspoken', 'ending—even', 'changed', 'into', 'autumn', '“but', 'know', 'small', 'mail,”', 'memorial,', 'lighter.', 'himself', 'meant', 'send', 'spoken', 'curls,', 'them,', 'hoped', 'was', 'wave.', 'person.', '9,', 'away', 'take', 'heaviness', 'of', 'calm.', 'cautious', 'yet.', 'looked', 'bouquet', 'i', 'with', 'weeks', 'walked', 'backed', 'see', 'argument,', 'bag.', 'story', 'not', 'stopped.', 'spilled', 'go,”', 'ago', 'words.', 'down', 'nodded,', 'shoulder.', 'gave', 'matter.', 'added.', 'us.”', 'envelope', 'he', 'headphones,', 'away,', '“it’s', 'something.', 'in.', 'lila.', 'want', 'as', 'students', 'don’t', 'to', 'met.', 'an', 'what', 'is', 'said.', 'it.”', 'on', 'we', 'when', 'trembled—not', '“i', 'chill', 'slipped', 'i’m', 'be', 'seen', 'memories', 'weight', 'that', 'mattered', 'doors', '“because', 'they', 'courage', 'he’d', 'finally', 'home', 'slammed', 'paused.', 'caught', 'had', 'coat', '“just', 'wanted', 'bubble', 'again.', 'letter.', 'speak.', 'last', 'anything,”', 'make', 'for.', 'said', 'between', 'cold', 'at', 'a', 'today,', 'people', 'was.', 'wrote', 'would', 'forever.', 'in', 'door,', 'inside', 'things', 'still', 'read', 'letter', 'around', 'pocket.', 'tonight,', 'week,', 'turn', 'his', 'long', 'slowly,', 'short', 'write', 'since', 'fingers', 'for', 'pounding.', 'nodded.', 'three', 'man', 'there', 'still,', 'next', 'platform', 'needs', 'out—families', 'over', 'mean.', 'pulling', 'long,”', 'crowd.', 'stood', 'suitcases,', 'same', 'ever—but', 'written', 'silence', 'but', 'smiled,', 'like', 'something,”', 'envelope.', '“thank', 'now?”', 'thumb', 'father’s', 'fog', 'turned', 'by', 'should', 'you…', 'tried.', 'wound,', 'heart', 'hadn’t', 'if', 'quickly', 'paper.', 'screeched', 'edge', 'didn’t', 'it.', 'swirled', 'late', 'followed', 'coming', 'him', 'said,', 'coat.', 'took', 'always', 'try', 'roses.', 'beginning,', 'smile,', 'lake.', 'found', 'quiet', 'her', 'lot', 'air,', '“of', 'words'}\n"
          ]
        }
      ],
      "source": [
        "lowercase_doc = document.lower()\n",
        "words = lowercase_doc.split()\n",
        "vocab = set(words)\n",
        "print(\"Number of words in the created string(document) is:\")\n",
        "print(len(words))\n",
        "print(\"Number of distinct words in the created string(document) is:\")\n",
        "print(len(vocab))\n",
        "print(\"Distinct words in the created string(document) are:\")\n",
        "print(\"*\"*100)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLXC6_vLbKq5"
      },
      "source": [
        "To make it easier to read, we could sort the vocabulary alphabetically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc_XT1nBbKq5"
      },
      "outputs": [],
      "source": [
        "sorted_vocab = sorted(vocab)\n",
        "print(sorted_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4ZzLRipp5KE"
      },
      "source": [
        "That looks a bit weird. What are all those bracket '(' characters doing there?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIB-6xDabKq5"
      },
      "source": [
        "### Removing punctuation with a regular expression\n",
        "\n",
        "Notice that many of the vocabulary terms, particularly those at the start of the list, contain punctuation characters like quotes '\"', brackets '(' and exclamation marks '!'. We'll now see how to remove these puntuation characters:\n",
        "- First get a list of punctuation characters from the 'string' library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCFQzseTbKq5"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wct3c1EAbKq6"
      },
      "source": [
        "The list is provided as a single string. To convert it to a list of individual characters, just call the list function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW-2BMVjbKq6"
      },
      "outputs": [],
      "source": [
        "list(string.punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxnuEuSjbKq6"
      },
      "source": [
        "Notice the double backslash character '\\\\\\\\' in the list. This is needed because backslash is used as the escape character. So if we don't put a double backslash, Python will interpret the single backslash as escaping the quote character that follows it.\n",
        "\n",
        "We can create a regular expression that will match any of those puncutation characters by simply surrounding the string of punctuation characters with square brackets: \"[]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSb8VgZrbKq6"
      },
      "outputs": [],
      "source": [
        "regex = '[' + string.punctuation + ']'\n",
        "print(regex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJK_3gYDbKq6"
      },
      "source": [
        "We can use the new punctuation matching regular expression with the sub() command in the *re* (regular expression) libarary to remove the unwanted punctuation.\n",
        "- Note that the sub() routine actually performs a substitution each time it finds a match, but we will simply replace the punctuation character with an empty string: ''\n",
        "- Let's print out the first 1000 characters of the text after removing all punctuation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRYPdI3sbKq7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "doc2_nopunctuation = re.sub(regex,'',doc2)\n",
        "print(doc2_nopunctuation[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDrmqJTbbKq7"
      },
      "source": [
        "Compare this output with the original text for the first 1000 characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-7aO0mxbKq7"
      },
      "outputs": [],
      "source": [
        "print(doc2[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBjg-w_pbKq7"
      },
      "source": [
        "Now that we've removed the punctuation, let's generate the sorted vocabulary again, by:\n",
        "- converting to lowercase\n",
        "- splitting on whitespace\n",
        "- select only distinct words\n",
        "- and sorting the words alphabetically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apHh-4PQbKq7"
      },
      "outputs": [],
      "source": [
        "words = doc2_nopunctuation.lower().split()\n",
        "sorted_vocab = sorted(set(words))\n",
        "print(sorted_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY6icNSJbKq7"
      },
      "source": [
        "### Counting term frequencies\n",
        "\n",
        "We often represent documents by their vocbulary, and in particular by their most frequently occuring terms, since those words are most likely to describe well the topic of the document.\n",
        "- We can count the frequency of the terms in the document using the Counter() function from the NLTK (Natural Language Tool Kit) library.\n",
        "- A online book describing the functionality that the NLTK library provides is available here: http://www.nltk.org/book/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJQwv6TpbKq8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "counts = nltk.Counter(words)\n",
        "print(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7_W-wMMbKq8"
      },
      "source": [
        "Note that the words are ordered according to their frequency.\n",
        "\n",
        "Lets display them again, but this time only the top 20, using the most_common() method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K5O9Jp7bKq8"
      },
      "outputs": [],
      "source": [
        "counts.most_common(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcxV1cjHbKq9"
      },
      "source": [
        "### Filtering Stopwords\n",
        "\n",
        "The most frequent terms: 'the', 'she', 'to', 'and', 'it', 'was', 'a', 'of', and 'i' aren't very interesting or descriptive of the story.\n",
        "- They are in fact frequent across *all documents* in the English language, and thus convey very little (if any) information about the topic of the document.\n",
        "- These terms are referred to as **'stop-words'**, because they can be removed from the description of the document without adversely affecting (indeed usually improving) the performance of a text search engine indexing the document.\n",
        "- The NLTK library contains lists of stop-word for English, Italian and many other languages. Let's print out the stop-word lists for English and Italian.\n",
        "\n",
        "Before we can get the stopword lists we need to download them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDz79vDebKq9"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnWF6C-KbKq9"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "print('English stopwords:')\n",
        "print(stopwords.words('english'))\n",
        "print()\n",
        "print('Italian stopwords:')\n",
        "print(stopwords.words('italian'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ0U8qCnbKq9"
      },
      "source": [
        "Now let's remove the stop-words from the tokenised text before counting the frequency of the words in the document.\n",
        "- We can easily remove items from a list using some special syntax in Python: **[x for x in list1 if x not in list2]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIHOmIwebKq9"
      },
      "outputs": [],
      "source": [
        "words_nostopwords = [w for w in words if w not in stopwords.words('english')]\n",
        "counts_nostopwords = nltk.Counter(words_nostopwords)\n",
        "counts_nostopwords.most_common(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R24uBJ9VbKq-"
      },
      "source": [
        "These words look a little bit better ...\n",
        "- The words 'Alice', 'time', 'eat', 'door' and 'rabbit' might be useful for describing the document\n",
        "- but many of the other words, like 'little', 'like', 'could' and 'get', migh not be as useful.\n",
        "\n",
        "\n",
        "To get an even better list of words for describing the document we would need to make use of information about *how common each word is in general in the English language*, since the more common a particular word is, the less likely it is to be useful for describing the document.\n",
        "- More on that later in the course ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu-ex9CDbKq-"
      },
      "source": [
        "## Downloading content from the Web\n",
        "\n",
        "One common source of text documents is the Web. Let's now download an article from Wikipedia, and then extract the text from it.\n",
        "\n",
        "First download the HTML page using the urllib library and print out just the first 2000 bytes of it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0BJs4iUbKq-"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "html_doc = urllib.request.urlopen('https://en.wikipedia.org/wiki/Dune_(novel)').read()\n",
        "html_doc[:2000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEroaWgRbKq-"
      },
      "source": [
        "Wow, that looks pretty ugly!\n",
        "\n",
        "Let's use another library (called Beautiful Soup) to parse the content of the page.\n",
        "- When printing out the parsed document we will use the prettify() method to indent all the HTML tags so that we can see the structure of the HTML document. (This is called 'pretty printing' in HTML/XML.)\n",
        "- Note that the printed output is very long, so after looking at it, you may want to edit the code to comment out the print line and re-run the cell.\n",
        " - To comment out the last line, simply place a hash character '#' in front of it: #print(parsed_doc.prettify())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0oHjPDNxbKq-",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import bs4 as bs\n",
        "parsed_doc = bs.BeautifulSoup(html_doc,'lxml')\n",
        "print(parsed_doc.prettify())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DniaRHMebKq_"
      },
      "source": [
        "### Extracting text from the HTML\n",
        "\n",
        "Now let's extract the text from the HTML page.\n",
        "- First find all paragraph \\<p\\> ... \\</p\\> elements within the HTML page.\n",
        "- The find_all() method returns a list of the elements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qetw2l_8bKq_"
      },
      "outputs": [],
      "source": [
        "paragraph_elements = parsed_doc.find_all('p')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GStEGbJrbKq_"
      },
      "source": [
        "Now print out the first of the paragraph elements to see what it looks like:\n",
        "- Note that Python starts counting from zero, not one, so the first element is: paragraph_elements[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chpj_uCTbKq_"
      },
      "outputs": [],
      "source": [
        "print(paragraph_elements[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j88DzCBDbKq_",
        "tags": []
      },
      "source": [
        "Well that was pretty uninteresting. The first paragraph was empty!\n",
        "- Print out the second paragraph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG1QIZ2LbKq_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(paragraph_elements[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaHrFocybKq_"
      },
      "source": [
        "OK, now let's get the text of each paragraph, without all of the HTML markup:\n",
        "- To do that we'll use the same python construct we saw before for iterating over the elements of a list.\n",
        "- This time though, we'll perform an operation on each element (extract the text) before returning the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ4e7kyVbKrA"
      },
      "outputs": [],
      "source": [
        "paragraph_texts = [p.text for p in paragraph_elements]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcgx_IwAbKrA"
      },
      "source": [
        "Print out the second paragraph to see how it looks without all of the HTML tags:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cepBdE6ObKrA"
      },
      "outputs": [],
      "source": [
        "print(paragraph_texts[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSkaGgtxbKrA"
      },
      "source": [
        "Print out the whole list to see text from the entire document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O38oIsntbKrA",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(paragraph_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLVjuK81bKrA"
      },
      "source": [
        "So there we have it, a list of paragraphs that have been extracted from a webpage.\n",
        "\n",
        "What shall we do with this text?\n",
        "- First let's join all the paragraphs together in a single string, separating them with a newline `\\n` character:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6LqUUP36bKrA",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "complete_text = '\\n'.join(paragraph_texts)\n",
        "print(complete_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYoolZfobKrB"
      },
      "source": [
        "### Searching within extracted text\n",
        "\n",
        "Now that we have the text in a convenient format, we can start doing some analysis on the it.\n",
        "- We could search for somebody's name, e.g. the author 'Frank Herbert', by using the `search` command from the regular expression package 're' imported above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-kI5ERqbKrB"
      },
      "outputs": [],
      "source": [
        "re.search('Frank Herbert', complete_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da5XgFHObKrB"
      },
      "source": [
        "This tells us that the author is first mentioned in between characters 256 and 269\n",
        "\n",
        "Let's find out how many times the director has been mentioned in the article. To do that we need to use the `findall()` command rather than search() command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlE3ND0bbKrB"
      },
      "outputs": [],
      "source": [
        "name = 'Frank Herbert'\n",
        "matches = re.findall(name, complete_text)\n",
        "print(matches)\n",
        "print(f\"The name '{name}' occurs {len(matches)} times\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM8y7KHxbKrB"
      },
      "source": [
        "More than just knowing that the author is being mentioned, we'd like to know what is being said about him. So we'd like to extract the sentences mentioning him.\n",
        "- We can do that by changing the regular expression that we are using to be more than just a string of keywords.\n",
        "\n",
        "The required regular expression is a little complicated, so let's build up to it slowly.\n",
        "- First let's write a simple expression to capture the first 10 characters immediately after his name.\n",
        "- In regular expressions, the dot character '.' is a wild-card that matches any character\n",
        "- so to match the next 10 characters, we can simply add ten dots to his name:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06Y1SEXQbKrB"
      },
      "outputs": [],
      "source": [
        "regex = name + '..........'\n",
        "print(f\"Regular expression: '{regex}'\")\n",
        "print(\"Returns:\")\n",
        "re.findall(regex, complete_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCZs2Aa0bKrB"
      },
      "source": [
        "That text window is far too short to be useful, and the regular expresssion is also particularly ugly.\n",
        "- Let's simplify regular expression by using the notation: `{n}` to repeat the previous character n times\n",
        "- and extend the window out to 100 characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frUSbdxabKrB"
      },
      "outputs": [],
      "source": [
        "regex = name + '.{100}'\n",
        "print(f\"Regular expression: '{regex}'\")\n",
        "print(\"Returns:\")\n",
        "re.findall(regex, complete_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ESWmmtAbKrB"
      },
      "source": [
        "Well the regular expression worked, but we lost one of the results because the required character window was too big.\n",
        "- A newline character was encountered less than 100 characters after the director's name.\n",
        "- To fix this, let's change the number of repetitions to be minimum zero, maximum 100:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrgXaV_ZbKrC"
      },
      "outputs": [],
      "source": [
        "regex = name + '.{,100}'\n",
        "re.findall(regex, complete_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rs0jgcPbKrC"
      },
      "source": [
        "OK, so that was fun, but what we'd really like to do is get the whole sentence around his name.\n",
        "- To do that we'll have to find all of the characters both before and after his name that do not include the period '.' character.\n",
        "- To choose any character except '.' we can write `[^.]` and to repeat that pattern zero or more times, we simply append '*'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NOpnAEebKrC"
      },
      "outputs": [],
      "source": [
        "regex = '[^.]*' + name + '[^.]*'\n",
        "re.findall(regex, complete_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beROSLPcbKrC"
      },
      "source": [
        "Finally, let's clean up the output a little:\n",
        "- by stripping off spaces and newline characters at the start of each sentence using the strip() method\n",
        "- and reappending the missing period at the end with `+ '.'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHg6gew9bKrC"
      },
      "outputs": [],
      "source": [
        "name = 'Frank Herbert'\n",
        "regex = '[^.]*' + name + '[^.]*'\n",
        "matches = re.findall(regex, complete_text)\n",
        "[m.strip() + '.' for m in matches]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zM2VdMxWapN"
      },
      "source": [
        "## Combine data from multiple files\n",
        "\n",
        "In some cases data sets contain many different information, as a result the content is split into different files:\n",
        "- We can open the required files through Python\n",
        "- We can load the required information using dictionaries for fast search over the data set\n",
        "- We can merge the data sets into strings to obtan the final data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hbz_3amWapN"
      },
      "source": [
        "### Loading files\n",
        "\n",
        "We are going to work with the [Cornell Movie--Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).\n",
        "You can download a copy of the original version of the corpus from this [link](http://www.mpi-sws.org/~cristian/data/cornell_movie_dialogs_corpus.zip).\n",
        "\n",
        "Extract the content of the zip archive and put it into a `docs/` directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SHdFvy_WapN"
      },
      "source": [
        "There are two main files in the corpus:\n",
        "- `movie_lines.txt` is a text file where each row is an utterance in a dialogue, it contains all the lines avaialble in the corpus\n",
        "- `movie_conversations.txt` is a text file where each row contains the list with the identifiers of the lines composing a dialogue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVo6PAnMWapN"
      },
      "source": [
        "First we load the utterances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXDUxIoeWapO"
      },
      "outputs": [],
      "source": [
        "with open('docs/cornell movie-dialogs corpus/movie_lines.txt') as f:\n",
        "    lines = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPe2bTm6WapO"
      },
      "source": [
        "Let's see what data we have in each row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dr0KrGNWapO"
      },
      "outputs": [],
      "source": [
        "print(lines[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZgWvwiaWapO"
      },
      "source": [
        "There are three elements we want to keep:\n",
        "- line identifier\n",
        "- speaker\n",
        "- utterance text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCI__BaXWapO"
      },
      "source": [
        "Then we load the dialogues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20-CDIeCWapP"
      },
      "outputs": [],
      "source": [
        "with open('docs/cornell movie-dialogs corpus/movie_conversations.txt') as f:\n",
        "    lines_list = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CDFb7OgWapP"
      },
      "source": [
        "Let's see what data we have in each row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k10-S-NDWapP"
      },
      "outputs": [],
      "source": [
        "print(lines_list[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfxzXx-0WapP"
      },
      "source": [
        "Here we are interested in keeping only the list of identifiers composing a dialogue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ryyyaJkWapP"
      },
      "source": [
        "### Parsing content with RegEx\n",
        "\n",
        "Now we can use RegEx to extract the useful information we want.\n",
        "\n",
        "With RegEx we can define the structure of an entire string or piece of string and we can group pieces of our expressions.\n",
        "In this way we can retreive specific pieces of a string that matches our request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naZHebQGWapQ"
      },
      "source": [
        "Each row in the lines file follows the same pattern:\n",
        "1. Line identifier\n",
        "2. Speaker identifier\n",
        "3. Movie identifier\n",
        "4. Speaker\n",
        "5. Utterance text\n",
        "\n",
        "We are interested in 1, 2, and 5.\n",
        "\n",
        "Note that we have a peculiar separator between the elements `+++$+++`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfSaMUJ-WapQ"
      },
      "source": [
        "Let's write a RegEx first and apply it to the first rows.\n",
        "\n",
        "We use round brakets `()` to isolate groups (groups can be nested)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4v_ZK56WapQ"
      },
      "outputs": [],
      "source": [
        "regex = '(L\\d+) \\+\\+\\+\\$\\+\\+\\+ u\\d+ \\+\\+\\+\\$\\+\\+\\+ m\\d+ \\+\\+\\+\\$\\+\\+\\+ ([\\w\\s]+) \\+\\+\\+\\$\\+\\+\\+ (.+)'\n",
        "print(f\"Regular expression: '{regex}'\")\n",
        "print(\"Returns:\")\n",
        "re.findall(regex, lines[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkUpqREqWapQ"
      },
      "source": [
        "What do you see in output?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wKLdQn8WapR"
      },
      "source": [
        "Now we can retrieve the desired information from each line and use it to build a list dictionaries where the keys are the IDs of the lines.\n",
        "\n",
        "Each element of the dictionary will contain the speaker and the uttered text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tHqch9nWapR"
      },
      "outputs": [],
      "source": [
        "lines = {key: {'speaker': sp, 'text': txt} for key, sp, txt in re.findall(regex, lines)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUpPJndGWapR"
      },
      "source": [
        "Now we can access the elements by specific names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g-jEg5YWapR"
      },
      "outputs": [],
      "source": [
        "print(type(lines))\n",
        "print(lines['L868'])\n",
        "print(type(lines['L868']))\n",
        "print(lines['L867']['speaker'])\n",
        "print(lines['L867']['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB_OozjfWapR"
      },
      "source": [
        "Now we can move to the dialogues file\n",
        "\n",
        "Each row in the dialogues file follows the same pattern:\n",
        "1. First speaker identifier\n",
        "2. Second speaker identifier\n",
        "3. Movie identifier\n",
        "4. List of lines identifier (expressed as a list of strings)\n",
        "\n",
        "We are interested only in 4.\n",
        "\n",
        "Note that we have again the peculiar separator between the elements `+++$+++`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ygdmjoWapS"
      },
      "source": [
        "We split the search into two parts, first isolate the lists and then retrieve elements from the lists. Let's write a RegEx first and apply it to the first rows.\n",
        "\n",
        "**NOTE: this is not the smartest way to appraoch it, but it is useful to understand how regex work**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NwvbOV5WapS"
      },
      "outputs": [],
      "source": [
        "list_regex = 'u\\d+ \\+\\+\\+\\$\\+\\+\\+ u\\d+ \\+\\+\\+\\$\\+\\+\\+ m\\d+ \\+\\+\\+\\$\\+\\+\\+ \\[(.+)\\]'\n",
        "print(f\"Regular expression: '{list_regex}'\")\n",
        "print(\"Returns:\")\n",
        "re.findall(list_regex, lines_list[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ITZzv-WapS"
      },
      "source": [
        "Each element here is a string with the code of the line. We can search in each string separately the line IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tqCn-3hWapS"
      },
      "outputs": [],
      "source": [
        "elem_regex = 'L\\d+'\n",
        "s = re.findall(list_regex, lines_list[:1000])[0]\n",
        "print(f\"Regular expression: '{elem_regex}'\")\n",
        "print(f\"String: \\\"{s}\\\"\")\n",
        "print(\"Returns:\")\n",
        "re.findall(elem_regex, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jGDOQEPWapS"
      },
      "source": [
        "Now we are dealing with an actual list of strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5yVCWVjWapT"
      },
      "outputs": [],
      "source": [
        "print(type(re.findall(elem_regex, s)))\n",
        "print(type(re.findall(elem_regex, s)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN3HPpl4WapT"
      },
      "source": [
        "Now we can retrieve the desired information from each line and use it to build a list, each element of the list is a list itself.\n",
        "The inner list contains the IDs of the lines composing the dialogue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PV1s7CrWapT"
      },
      "outputs": [],
      "source": [
        "lines_list = [re.findall(elem_regex, s) for s in re.findall(list_regex, lines_list)]\n",
        "lines_list[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v99m8vsWapT"
      },
      "source": [
        "### Composing the dialogues\n",
        "\n",
        "Now we have an indexed list of lines and a list of IDs composing a dialogue, we can finally put all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRdZRwK1WapU"
      },
      "source": [
        "For each dialogue in `lines_list` we compose a string with all the turns separated by a newline character.\n",
        "A turn is a speaker-text pair in the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyw0V3u9WapU"
      },
      "outputs": [],
      "source": [
        "dialogues = [\n",
        "    '\\n'.join(f'{lines[idx][\"speaker\"]}: {lines[idx][\"text\"]}' for idx in indices)\n",
        "    for indices in lines_list if all(idx in lines for idx in indices)  # There are some missing\n",
        "]\n",
        "len(dialogues)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adW8wds-WapU"
      },
      "source": [
        "We can give a look to one of the extracted dialogues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFiyNdDjWapU"
      },
      "outputs": [],
      "source": [
        "print(dialogues[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH9T6tWNWapU"
      },
      "outputs": [],
      "source": [
        "print(dialogues[2307])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzOu1jlTbKrC",
        "tags": []
      },
      "source": [
        "## Loading text from a PDF\n",
        "\n",
        "Much of the text on the internet is present inside PDF documents, and often we'd like to extract text from them.\n",
        "\n",
        "There are many different ways to do that in Python. Today we'll use the pdfplumber API: https://github.com/jsvine/pdfplumber\n",
        "- In order to use pdfplumber module, we first need to install it.\n",
        "- We can do that from inside the jupyter notebook by calling the pip3 command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ_Q1rEPbKrC"
      },
      "outputs": [],
      "source": [
        "!pip3 install pdfplumber"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sic3jHSObKrC"
      },
      "source": [
        "Now we can import the module and use it to extract content from a PDF.\n",
        "- Let's try extracting content from this NLP reasearch paper: https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf\n",
        "- The HTTPS server won't allow us direct programmatic access, so you'll need to use the file in the `docs/` directory on WeBeep where you found this notebook (as we did with the original Alice text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN-4B_UIbKrD"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "filename = 'docs/collobert11a.pdf'\n",
        "pdf = pdfplumber.open(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C49W1ARdbKrD"
      },
      "source": [
        "How many pages are in the report?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzALGQywbKrD"
      },
      "outputs": [],
      "source": [
        "len(pdf.pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZXe04dobKrD"
      },
      "source": [
        "Wow, that's not a lot of pages!\n",
        "\n",
        "We can have a look at the first couple of pages extracting the text from them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM4yYdfjbKrD"
      },
      "outputs": [],
      "source": [
        "text = pdf.pages[0].extract_text(x_tolerance=1)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXRuv5mibKrD"
      },
      "outputs": [],
      "source": [
        "text = pdf.pages[1].extract_text(x_tolerance=1)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oNjvSm1bKrD"
      },
      "source": [
        "Extract the text from all the pages of the document into a list\n",
        "- Note: this might take a minute. There are a lot of pages ;-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCuTTuvSbKrD"
      },
      "outputs": [],
      "source": [
        "texts = [page.extract_text(x_tolerance=1) for page in pdf.pages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFdEPAa0bKrD"
      },
      "source": [
        "Now concatenate all the text together into a single string.\n",
        "- We'll separate them from one another using a couple of newline characters and some spaces too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VA2Je3JpbKrE",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "text = \"  \\n\\n\".join(texts)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8XeouXkbKrE"
      },
      "source": [
        "#### Using Regular Expressions to search PDF\n",
        "\n",
        "Use some regular expressions to search through the text for some interesting content.\n",
        "- You could look for email addresses, phone numbers, addresses, ...\n",
        "- Let's try first to look for email addresses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXMhf0aNbKrE"
      },
      "outputs": [],
      "source": [
        "regex = '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "emails = re.findall(regex,text)\n",
        "print(emails)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqzxkQ28bKrE"
      },
      "source": [
        "Did you find any?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5XPlU-1WapY"
      },
      "source": [
        "POS, Chunking, NER, and SRL are all NLP tasks.\n",
        "- Are they mentioned anywhere in the paper?\n",
        "- Write a regular expression to find out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JabBh00YbKrE"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-eWNcQbbKrE"
      },
      "source": [
        "Other ideas to try:\n",
        "- In this period reaserchers started using neural networks to solve NLP. Find out where neural networks are mentioned in the report and in what context.\n",
        "- Theauthors use a data set composed of text coming from the Wall Street Journal (WSJ). Search for references to it in the PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPGXsTFFbKrE"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBR_N4eQbKrE"
      },
      "source": [
        "Load another PDF and write regular expressions to search for content in it.\n",
        "- For example, you can find reports for Ferrari here: https://corporate.ferrari.com/en/investors/results/reports\n",
        "- Let's load an interim report from September 2020 (you can find it in the same `docs` folder as before):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zzSw45EtbKrF",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "filename = 'docs/ferrari_interim_report_at_and_for_the_three_and_nine_months_ended_september_30_2020.pdf'\n",
        "pdf = pdfplumber.open(filename)\n",
        "text = '\\n\\n'.join([page.extract_text() for page in pdf.pages])\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd8hAX4aV0JK"
      },
      "source": [
        "### Extracting Tables from a PDF\n",
        "\n",
        "Sometimes it can be useful to extract tabular data from a PDF.\n",
        "- Tools exist that allow you to do this programmatically, making the extraction process semi-automatic.\n",
        "- One tool that can do this is the *tabula* library. Let's install it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_rwPuYmGCVc"
      },
      "outputs": [],
      "source": [
        "!pip install tabula-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naFDi4yNWyY9"
      },
      "source": [
        "Now we can use *tabula* to extract all the tables from Ferrari's interim report above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pQwzInjG8Rt"
      },
      "outputs": [],
      "source": [
        "import tabula\n",
        "tables = tabula.read_pdf(filename, pages=\"all\", multiple_tables=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNrQUIGrX6Rn"
      },
      "source": [
        "Let's have a look at some of the tables produced\n",
        "- the first table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86E4O5vDH8Hq"
      },
      "outputs": [],
      "source": [
        "tables[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXPN-2SDYCao"
      },
      "source": [
        "- the third table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fOAuloxYKvp"
      },
      "outputs": [],
      "source": [
        "tables[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDCG8UeAYbwM"
      },
      "source": [
        "It can be seen that the tables are in need of a bit of cleaning to make them usable.\n",
        "- The tables are Pandas dataframes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWqHAlmoJSaS"
      },
      "outputs": [],
      "source": [
        "type(tables[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reWHSqBOYwcU"
      },
      "source": [
        "So we can clean-up the table by:\n",
        "- dropping some columns\n",
        "- dropping some rows\n",
        "- renaming the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlpIdJI0Jt-n"
      },
      "outputs": [],
      "source": [
        "df = tables[0]\n",
        "df = df.drop(df.columns[[1,5]], axis=1)  # drop columns: 1,5\n",
        "df = df.drop([0,5,12])                   # drop rows: 0,5,12\n",
        "df = df.reset_index(drop=True)           # reset index\n",
        "df.columns = ('Field','3months_to_30092020','3months_to_30092019','9months_to_30092020','9months_to_30092019') # rename columns\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUsE_YcMbjZ_"
      },
      "source": [
        "During the cleaning phase, some of the values may need to be updated too (e.g. certain values in the Field column above).\n",
        "- Ideally the above cleaning operations would be done automatically.\n",
        "- In practice, tables have lots of nested structure (including the one we just extracted),\n",
        "- and it's still a hard research problem to do the cleaning reliably, (particularly the generation of the column names that we provided manually).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udj4WngyWapb"
      },
      "source": [
        "## Loading text from scanned document\n",
        "\n",
        "But what if my documents have been scanned?\n",
        "- In that case the task of extracting text from them is much more difficult.\n",
        "\n",
        "It is possible to extract text also from images, but you will need to have an Optical Character Recognition (OCR) system installed.\n",
        "We can use a combination of layout parsing and OCR to extract the text.\n",
        "- Layout parser is an opensource library to detect leyoutis in images: https://towardsdatascience.com/analyzing-document-layout-with-layoutparser-ed24d85f1d44\n",
        "- Tesseract is an opensource OCR system provided by Google. Some systems (such as Linux) come with Tesseract pre-installed. Others need to install it from here: https://tesseract-ocr.github.io/tessdoc/Home.html\n",
        "- If you have Tesseract installed, you can follow the instructions here to use it from Python: https://towardsdatascience.com/extracting-text-from-scanned-pdf-using-pytesseract-open-cv-cd670ee38052\n",
        "\n",
        "We can take out a page from this paper on speech synthesis, convert it to PNG and scan it: http://proceedings.mlr.press/v80/skerry-ryan18a/skerry-ryan18a.pdf\n",
        "- Let's take page 4, which contains also figures"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting, we will install the required packages"
      ],
      "metadata": {
        "id": "yELMoQ_LcOHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install layoutparser torchvision && pip3 install \"detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.5#egg=detectron2\"\n",
        "!pip3 install \"layoutparser[ocr]\""
      ],
      "metadata": {
        "id": "GzsfKKdGe2wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn"
      ],
      "metadata": {
        "id": "a8bLUL_LjHVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the image as a NumPy array"
      ],
      "metadata": {
        "id": "AS0qMxNUddU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "\n",
        "img = imageio.imread('docs/skerry-ryan18a - pag 4.jpg')\n",
        "print(img.shape)"
      ],
      "metadata": {
        "id": "uSP2mS94cMGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a document ready for layout detection.\n",
        "\n",
        "Let's use layout parser to detect the document structure. For of all we need to load the model."
      ],
      "metadata": {
        "id": "hZEQmb09gDMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import layoutparser as lp\n",
        "\n",
        "model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n",
        "                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n",
        "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})"
      ],
      "metadata": {
        "id": "yUtBppJocLcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can detect the layout. Let's see what output LayoutParser generates."
      ],
      "metadata": {
        "id": "kDUxHWwthNiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layout_result = model.detect(img)\n",
        "layout_result"
      ],
      "metadata": {
        "id": "q5RForqpcLGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's not very helpful, we can try to visualise the results:"
      ],
      "metadata": {
        "id": "MFlsNkgjhehx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lp.draw_box(img, layout_result,  box_width=5, box_alpha=0.2, show_element_type=True)\n"
      ],
      "metadata": {
        "id": "hPB-XYPvcLCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's not perfect but it's really nice don't you think?\n",
        "\n",
        "It even recognises figures and titles, especially the second is non-trivial (and it fact there is an error).\n",
        "\n",
        "Now what if we want to retain only the text regions?"
      ],
      "metadata": {
        "id": "UBRQ7VTghsv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_blocks = lp.Layout([b for b in layout_result if b.type=='Text'])\n",
        "lp.draw_box(img, text_blocks,  box_width=5, box_alpha=0.2, show_element_type=True, show_element_id=True)"
      ],
      "metadata": {
        "id": "bK0pixzucK-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the OCR tool to extract the identifies text. We need just a bit of preprocessing to retain the blocks in order."
      ],
      "metadata": {
        "id": "fgSP5gzniMvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_width = len(img[0])\n",
        "\n",
        "# Sort element ID of the left column based on y1 coordinate\n",
        "left_interval = lp.Interval(0, image_width/2, axis='x').put_on_canvas(img)\n",
        "left_blocks = text_blocks.filter_by(left_interval, center=True)._blocks\n",
        "left_blocks.sort(key = lambda b:b.coordinates[1])\n",
        "\n",
        "# Sort element ID of the right column based on y1 coordinate\n",
        "right_blocks = [b for b in text_blocks if b not in left_blocks]\n",
        "right_blocks.sort(key = lambda b:b.coordinates[1])\n",
        "\n",
        "# Sort the overall element ID starts from left column\n",
        "text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])"
      ],
      "metadata": {
        "id": "YdEf9cH0cK6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create an instance of the OCR agent and run it on the text blocks"
      ],
      "metadata": {
        "id": "BI6R9ZpGigS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ocr_agent = lp.TesseractAgent(languages='eng')"
      ],
      "metadata": {
        "id": "j-giCNzdcK2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for block in text_blocks:\n",
        "\n",
        "    # Crop image around the detected layout\n",
        "    segment_image = (block.pad(left=15, right=15, top=5, bottom=5).crop_image(img))\n",
        "\n",
        "    # Perform OCR\n",
        "    text = ocr_agent.detect(segment_image)\n",
        "\n",
        "    # Save OCR result\n",
        "    block.set(text=text, inplace=True)"
      ],
      "metadata": {
        "id": "xFKRzLb-iuYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we ca read the text we extracted from our blocks:"
      ],
      "metadata": {
        "id": "bHaOhsp8j-Tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for txt in text_blocks:\n",
        "    print(txt.text, end='\\n\\n')"
      ],
      "metadata": {
        "id": "hMDt2GuXcKx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a string we can have fun using all the RegEx we want to extract information."
      ],
      "metadata": {
        "id": "jRJ9IUx0kImT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HELmS9qTkEid"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H9fk15RybKqy",
        "hOyZfFYFeYvf",
        "NlH3zCkHbKq0",
        "z4fWcIzthAya",
        "3Bwb2-TTbKq2"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}